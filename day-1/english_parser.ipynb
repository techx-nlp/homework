{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Parser\n",
    "\n",
    "\"BuT dAvId, ThAt's JuSt A mOnOiDaL-eNdOfUnCtOr-DeRiVeD cOnTeXt-CoErCiNg LaMbDa CoMbInAtOr In SiMpLeR tErMs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import enum\n",
    "\n",
    "from xnlp.parser import (\n",
    "    Parser,\n",
    "    runParser,\n",
    "    satisfy,\n",
    "    optional,\n",
    "    zero_or_more,\n",
    "    one_or_more,\n",
    "    expect_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are going to explore how to write parsing rules as described in class. The grammar you are going to design is a simplified version of English; words in such language consist of only 6 types:\n",
    "\n",
    "- `ADJ`: Adjectives (e.g. `beautiful`, `big`)\n",
    "- `ADV`: Adverbs (e.g. `gracefully`, `quickly`)\n",
    "- `NOUN`: Noun (e.g. `penguin`, `penguin`, `penguin`)\n",
    "- `PUNCT`: Punctuation (e.g. `,`, `;`, `.`)\n",
    "- `VERB`: Verb (e.g. `run`, `eat`, `slap`)\n",
    "- `CCONJ`: Coordinating Conjunctions (e.g. `but`, `and`, `yet`)\n",
    "\n",
    "For a more in-depth explanation on the above part-of-speech tags, visit [Universal POS Tags](https://universaldependencies.org/u/pos/all.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T(enum.Enum):\n",
    "    \"\"\"\n",
    "    Some simple tokens. Very inconclusive, but simple.\n",
    "    \"\"\"\n",
    "\n",
    "    ADJ = 'Adjective'\n",
    "    ADV = 'Adverb'\n",
    "    NOUN = 'Noun'\n",
    "    PUNCT = 'Punctuation'\n",
    "    VERB = 'Verb'\n",
    "    CCONJ = 'Coordinating Conjunction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse each sentence, we will be using monadic parser combinators to make life easier. Here is some setup code. Feel free to skip if you are not interested.\n",
    "\n",
    "Note that the parse result will be linearized into a list for simplicity's sake. This assignment focuses more on seeing if your parser can identify grammatically correct sentences from those with grammatical errors than building the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_satisfy = lambda p: satisfy(p).fmap(lambda t: t[0])\n",
    "word = lambda w: wrapped_satisfy(lambda x: x[0] == w)\n",
    "tok = lambda t: wrapped_satisfy(lambda x: x[1] == t)\n",
    "end = expect_end('Expecting end of input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens in a sentence is already tokenized for you. Tokens are structured as tuples:\n",
    "\n",
    "```python\n",
    "('word', T.NOUN)\n",
    "```\n",
    "\n",
    "__Tip__: for the most part, you only need to check for the token's type when constructing the parser; the only place where the string content is necessary is when an `Token.PUNCT` is encountered.\n",
    "\n",
    "Test sentences are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE = [\n",
    "    ('Elephant', T.NOUN),\n",
    "    ('eat', T.VERB),\n",
    "    ('banana', T.NOUN),\n",
    "    ('.', T.PUNCT)\n",
    "]\n",
    "\n",
    "SENTS = [\n",
    "    SIMPLE\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the parser is rather simple; you just need to organize everything into Backus-Naur Form like we discussed during class. To illustrate, here is how you parse a simple noun-verb-noun statement (`+` is the concatenating separator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUN_V_NOUN = tok(T.NOUN) + tok(T.VERB) + tok(T.NOUN) + word('.') + end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try parsing the simple \"Elephant eat banana.\" sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: ['Elephant', 'eat', 'banana', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Result:', runParser(NOUN_V_NOUN, SIMPLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that easy, yay!\n",
    "\n",
    "You might realize the similarity between our combinators and the BNF syntax; indeed, they captures the same concept. Here are some other tools:\n",
    "\n",
    "- `optional`: optionally matches a parser (e.g. `optional(tok(T.NOUN))` matches a `Noun` and returns `[<NOUN>]` if possible, otherwise returns `[]`)\n",
    "- `zero_or_more`: matches a parser as much times as possible, including empty match (e.g. `zero_or_more(tok(T.NOUN))` matches numerous `Noun` and return `[<NOUN> * n]`, where `n` can be `0`)\n",
    "- `one_or_more`: matches a parser as much times as possible, but at least once\n",
    "- `end`: matches the end of an input\n",
    "- `+`: concatenates two parsers; they must match the input string in the given order\n",
    "- `|`: joins two parsers; the new parser will success if either of the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
